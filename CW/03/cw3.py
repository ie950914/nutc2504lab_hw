import os
import csv
import requests
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# --- 1. é…ç½®èˆ‡è·¯å¾‘è¨­å®š ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
LLM_API_URL = "https://ws-02.wade0426.me/v1/chat/completions"
LLM_MODEL = "gemma-3-27b-it"

COLLECTION_NAME = "CW_03" 
CHUNK_SIZE = 500  # ç¨å¾®åŠ å¤§åˆ‡å¡Šï¼Œè®“ Context æ›´å®Œæ•´
CHUNK_OVERLAP = 50

def get_embedding(texts):
    """å–å¾—å‘é‡èˆ‡ç¶­åº¦"""
    try:
        res = requests.post(EMBED_API_URL, json={
            "texts": texts, "task_description": "æª¢ç´¢æ–‡ä»¶", "normalize": True
        }, timeout=30).json()
        embs = res.get("embeddings", [])
        return embs, len(embs[0]) if embs else 0
    except Exception as e:
        print(f"âŒ Embedding éŒ¯èª¤: {e}")
        return None, 0

def call_llm(system_prompt, user_prompt):
    """å‘¼å« LLM API"""
    try:
        res = requests.post(LLM_API_URL, json={
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1
        }, timeout=60).json()
        return res["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âŒ LLM å‘¼å«å¤±æ•—: {e}")
        return ""

def main():
    # é€£æ¥ Qdrant (è«‹ç¢ºä¿ sudo docker å·²å•Ÿå‹•)
    client = QdrantClient("localhost", port=6333)
    
    # --- A. æº–å‚™ VDB ---
    print(f"ğŸš€ åˆå§‹åŒ– VDB: {COLLECTION_NAME}")
    _, dim = get_embedding(["æ¸¬è©¦"])
    if dim == 0: 
        print("âŒ ç„¡æ³•åµæ¸¬ç¶­åº¦ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ– API URL"); return
        
    if client.collection_exists(COLLECTION_NAME): 
        client.delete_collection(COLLECTION_NAME)
    client.create_collection(COLLECTION_NAME, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))

    # --- B. åˆ‡å¡Šèˆ‡åŒ¯å…¥è³‡æ–™ ---
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    all_points = []
    p_idx = 0
    
    # æœå°‹åŒè³‡æ–™å¤¾ä¸‹çš„ data_01.txt ~ data_05.txt
    for i in range(1, 6):
        path = os.path.join(SCRIPT_DIR, f"data_0{i}.txt")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
                chunks = splitter.split_text(content)
                embs, _ = get_embedding(chunks)
                if embs:
                    for c, e in zip(chunks, embs):
                        all_points.append(PointStruct(id=p_idx, vector=e, payload={"text": c, "source": f"data_0{i}.txt"}))
                        p_idx += 1
    
    if all_points:
        client.upsert(COLLECTION_NAME, all_points)
        print(f"âœ… å·²å­˜å…¥ {p_idx} å€‹èªæ„å¡Šè‡³ Qdrant")
    else:
        print("âŒ æ‰¾ä¸åˆ° data_*.txt æª”æ¡ˆï¼Œè«‹æª¢æŸ¥æª”æ¡ˆåç¨±èˆ‡ä½ç½®")

    # --- C. è™•ç† CSV å•é¡Œé›† (Query Re-Write æ ¸å¿ƒ) ---
    input_path = os.path.join(SCRIPT_DIR, "Re_Write_questions.csv")
    if not os.path.exists(input_path):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”: {input_path}"); return

    with open(input_path, "r", encoding="utf-8-sig") as f:
        rows = list(csv.DictReader(f))

    # æŒ‰ conversation_id åˆ†çµ„ï¼Œç¢ºä¿æ­·å²å°è©±é‚è¼¯æ­£ç¢º
    conv_groups = {}
    for r in rows:
        cid = r['conversation_id']
        if cid not in conv_groups: conv_groups[cid] = []
        conv_groups[cid].append(r)

    final_results = []
    for cid, questions in conv_groups.items():
        history = "" # æ¯å€‹æ–° Session é‡ç½®å°è©±æ­·å²
        print(f"\nğŸ“‚ æ­£åœ¨è™•ç† Session: {cid}")
        
        for q in questions:
            user_q = q['questions'] # æ³¨æ„é€™è£¡å°æ‡‰ CSV æ¬„ä½åç¨±
            
            # 1. Query Re-Write
            if not history:
                search_query = user_q # ç¬¬ä¸€é¡Œç›´æ¥æœå°‹
            else:
                rewrite_sys = "ä½ æ˜¯ä¸€å€‹æŸ¥è©¢é‡å¯«å°ˆå®¶ã€‚è«‹æ ¹æ“šå°è©±æ­·å²ï¼Œå°‡ä½¿ç”¨è€…çš„æœ€æ–°å•é¡Œæ”¹å¯«æˆä¸€å€‹èªæ„å®Œæ•´ä¸”é©åˆæœå°‹æŠ€è¡“æ–‡ä»¶çš„ç¨ç«‹å¥å­ã€‚åš´ç¦è§£é‡‹æˆ–å»¢è©±ã€‚"
                rewrite_usr = f"æ­·å²ï¼š{history}\næœ€æ–°å•é¡Œï¼š{user_q}\né‡å¯«å¾Œçš„æœå°‹å¥ï¼š"
                search_query = call_llm(rewrite_sys, rewrite_usr).split('\n')[0].replace('"', '')
            
            print(f"   ğŸ” åŸå§‹: {user_q[:15]}... -> æœå°‹å¥: {search_query}")

            # 2. æª¢ç´¢ (Retrieval)
            q_emb, _ = get_embedding([search_query])
            hits = client.query_points(COLLECTION_NAME, query=q_emb[0], limit=3).points
            
            context = "\n".join([h.payload["text"] for h in hits])
            source = hits[0].payload["source"] if hits else "æœªçŸ¥"

            # 3. å›ç­”ç”Ÿæˆ (RAG)
            ans_sys = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„åƒè€ƒè³‡æ–™ï¼Œç²¾æº–ä¸”ç°¡çŸ­åœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹å›ç­”ã€Œè³‡æ–™åº«ç„¡ç›¸é—œè¨˜è¼‰ã€ã€‚"
            ans_usr = f"ã€åƒè€ƒè³‡æ–™ã€‘ï¼š\n{context}\n\nã€å•é¡Œã€‘ï¼š{user_q}"
            answer = call_llm(ans_sys, ans_usr)

            # æ›´æ–°çµæœèˆ‡æ­·å²
            q.update({"answer": answer, "source": source})
            final_results.append(q)
            # ç°¡çŸ­ç´€éŒ„æ­·å²ä¾›ä¸‹æ¬¡é‡å¯«ä½¿ç”¨
            history += f" Q:{user_q} A:{answer[:10]}"

    # --- D. å¯«å›çµæœ ---
    out_path = os.path.join(SCRIPT_DIR, "Re_Write_results.csv")
    with open(out_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(final_results)
    
    print(f"\nğŸ‰ è™•ç†å®Œæˆï¼çµæœå·²å­˜è‡³: {out_path}")

if __name__ == "__main__":
    main()