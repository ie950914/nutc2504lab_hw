import requests
import pandas as pd
import re
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ============================================
# è¨­å®šèˆ‡åˆå§‹åŒ–
# ============================================

API_EMBED_URL = "https://ws-04.wade0426.me/embed"
QDRANT_URL = "http://localhost:6333"


# é€£æ¥ Qdrant
try:
    client = QdrantClient(url=QDRANT_URL)
    print("âœ… å·²æˆåŠŸé€£æ¥è‡³ Qdrant VDB")
except Exception as e:
    print(f"âŒ ç„¡æ³•é€£æ¥ Qdrant: {e}")
    exit()

# ============================================
# å·¥å…·å‡½æ•¸
# ============================================

def get_embeddings(texts):
    """å–å¾—æ–‡æœ¬å‘é‡"""
    response = requests.post(API_EMBED_URL, json={
        "texts": texts,
        "normalize": True,
        "batch_size": 32
    })
    if response.status_code == 200:
        return response.json()
    else:
        print(f"âŒ å‘é‡ç”Ÿæˆå¤±æ•—: {response.status_code}")
        return None

def markdown_to_csv(md_file, csv_file):
    """Markdown è¡¨æ ¼è½‰ CSV"""
    with open(md_file, 'r', encoding='utf-8') as f:
        content = f.read()

    lines = content.strip().split('\n')
    data = []

    for line in lines:
        # è·³éè¡¨æ ¼åˆ†éš”ç·š
        if '|' not in line or line.strip() == '':
            continue
        # è·³éåªæœ‰åˆ†éš”ç¬¦çš„è¡Œ
        if all(c in '|-: \t' for c in line.replace('|', '')):
            continue
        
        cells = [cell.strip() for cell in line.split('|')]
        cells = [c for c in cells if c]
        if cells:
            data.append(cells)

    if len(data) > 1:
        df = pd.DataFrame(data[1:], columns=data[0])
        df.to_csv(csv_file, index=False, encoding='utf-8')
        return df
    else:
        return None

# ============================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æœ¬åˆ‡å¡Šèˆ‡åµŒå…¥
# ============================================

print("\n" + "="*60)
print("ç¬¬ä¸€éƒ¨åˆ†ï¼šæ–‡æœ¬åˆ‡å¡Šè™•ç†")
print("="*60)

# è®€å–æ–‡æœ¬æ–‡ä»¶
with open("text.txt", "r", encoding="utf-8") as f:
    text = f.read()

print(f"ğŸ“„ åŸå§‹æ–‡æœ¬é•·åº¦: {len(text)} å­—ç¬¦")

# 1. å›ºå®šåˆ‡å¡Š (ç„¡é‡ç–Š)
print("\nã€å›ºå®šåˆ‡å¡Šã€‘")
fixed_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=0,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
)

fixed_chunks = fixed_splitter.split_text(text)
print(f"âœ… å›ºå®šåˆ‡å¡Šç”¢ç”Ÿ {len(fixed_chunks)} å€‹æ–¹å¡Š")

# 2. æ»‘å‹•è¦–çª—åˆ‡å¡Š (æœ‰é‡ç–Š)
print("\nã€æ»‘å‹•è¦–çª—åˆ‡å¡Šã€‘")
sliding_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
)

sliding_chunks = sliding_splitter.split_text(text)
print(f"âœ… æ»‘å‹•è¦–çª—åˆ‡å¡Šç”¢ç”Ÿ {len(sliding_chunks)} å€‹æ–¹å¡Š")

# 3. ç”Ÿæˆå‘é‡åµŒå…¥
print("\nğŸ“Š é–‹å§‹ç”Ÿæˆå‘é‡åµŒå…¥...")

# å›ºå®šåˆ‡å¡ŠåµŒå…¥
fixed_data = {
    "texts": fixed_chunks,
    "normalize": True,
    "batch_size": 32
}
fixed_response = requests.post(API_EMBED_URL, json=fixed_data)

if fixed_response.status_code == 200:
    fixed_result = fixed_response.json()
    print(f"âœ… å›ºå®šåˆ‡å¡Šå‘é‡ç¶­åº¦: {fixed_result['dimension']}")
else:
    print(f"âŒ å›ºå®šåˆ‡å¡ŠåµŒå…¥å¤±æ•—")
    exit()

# æ»‘å‹•è¦–çª—åˆ‡å¡ŠåµŒå…¥
sliding_data = {
    "texts": sliding_chunks,
    "normalize": True,
    "batch_size": 32
}
sliding_response = requests.post(API_EMBED_URL, json=sliding_data)

if sliding_response.status_code == 200:
    sliding_result = sliding_response.json()
    print(f"âœ… æ»‘å‹•è¦–çª—åˆ‡å¡Šå‘é‡ç¶­åº¦: {sliding_result['dimension']}")
else:
    print(f"âŒ æ»‘å‹•è¦–çª—åˆ‡å¡ŠåµŒå…¥å¤±æ•—")
    exit()

# 4. å­˜å…¥ Qdrant
print("\nğŸ’¾ é–‹å§‹å­˜å…¥ Qdrant...")

# å»ºç«‹å›ºå®šåˆ‡å¡Šé›†åˆ
if client.collection_exists("fixed_collection"):
    client.delete_collection("fixed_collection")

client.create_collection(
    collection_name="fixed_collection",
    vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
)

# å»ºç«‹æ»‘å‹•è¦–çª—åˆ‡å¡Šé›†åˆ
if client.collection_exists("sliding_collection"):
    client.delete_collection("sliding_collection")

client.create_collection(
    collection_name="sliding_collection",
    vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
)

# æ’å…¥å›ºå®šåˆ‡å¡Šå‘é‡
fixed_points = []
for i, vec in enumerate(fixed_result['embeddings']):
    fixed_points.append(
        PointStruct(
            id=i + 1,
            vector=vec,
            payload={"text": fixed_chunks[i], "chunk_type": "fixed", "chunk_id": i}
        )
    )

client.upsert(collection_name="fixed_collection", points=fixed_points)
print(f"âœ… æˆåŠŸæ’å…¥ {len(fixed_points)} å€‹å›ºå®šåˆ‡å¡Šå‘é‡")

# æ’å…¥æ»‘å‹•è¦–çª—åˆ‡å¡Šå‘é‡
sliding_points = []
for i, vec in enumerate(sliding_result['embeddings']):
    sliding_points.append(
        PointStruct(
            id=i + 1,
            vector=vec,
            payload={"text": sliding_chunks[i], "chunk_type": "sliding", "chunk_id": i}
        )
    )

client.upsert(collection_name="sliding_collection", points=sliding_points)
print(f"âœ… æˆåŠŸæ’å…¥ {len(sliding_points)} å€‹æ»‘å‹•è¦–çª—åˆ‡å¡Šå‘é‡")

# ============================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå¬å›æ¸¬è©¦èˆ‡æ¯”è¼ƒ
# ============================================

print("\n" + "="*60)
print("ç¬¬äºŒéƒ¨åˆ†ï¼šå¬å›æ¸¬è©¦èˆ‡æ¯”è¼ƒ")
print("="*60)

# æ¸¬è©¦å•é¡Œ
test_queries = [
    "Graph RAG æœ‰ä»€éº¼å„ªå‹¢?",
    "çŸ¥è­˜åœ–è­œå¦‚ä½•å»ºæ§‹?",
    "å¾®è»Ÿ GraphRAG çš„ç‰¹é»æ˜¯ä»€éº¼?"
]

for query_text in test_queries:
    print(f"\nğŸ” æŸ¥è©¢å•é¡Œ: {query_text}")
    print("-"*60)
    
    # ç”ŸæˆæŸ¥è©¢å‘é‡
    query_data = {
        "texts": [query_text],
        "normalize": True,
        "batch_size": 32
    }
    query_response = requests.post(API_EMBED_URL, json=query_data)
    
    if query_response.status_code != 200:
        print("âŒ æŸ¥è©¢å‘é‡ç”Ÿæˆå¤±æ•—")
        continue
    
    query_vector = query_response.json()['embeddings'][0]
    
    # å›ºå®šåˆ‡å¡ŠæŸ¥è©¢
    fixed_search = client.query_points(
        collection_name="fixed_collection",
        query=query_vector,
        limit=3
    )
    
    # æ»‘å‹•è¦–çª—åˆ‡å¡ŠæŸ¥è©¢
    sliding_search = client.query_points(
        collection_name="sliding_collection",
        query=query_vector,
        limit=3
    )
    
    # æ¯”è¼ƒæœ€é«˜åˆ†æ•¸
    fixed_max_score = max([p.score for p in fixed_search.points]) if fixed_search.points else 0
    sliding_max_score = max([p.score for p in sliding_search.points]) if sliding_search.points else 0
    
    print(f"\nğŸ“Š å›ºå®šåˆ‡å¡Šæœ€é«˜åˆ†: {fixed_max_score:.4f}")
    print(f"   æœ€ä½³çµæœ: {fixed_search.points[0].payload['text'][:80]}...")
    
    print(f"\nğŸ“Š æ»‘å‹•è¦–çª—æœ€é«˜åˆ†: {sliding_max_score:.4f}")
    print(f"   æœ€ä½³çµæœ: {sliding_search.points[0].payload['text'][:80]}...")
    
    winner = "æ»‘å‹•è¦–çª—" if sliding_max_score > fixed_max_score else "å›ºå®šåˆ‡å¡Š"
    print(f"\nğŸ† æœ¬æ¬¡æŸ¥è©¢ç²å‹: {winner}")

# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¡¨æ ¼è™•ç†
# ============================================

print("\n" + "="*60)
print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¡¨æ ¼è™•ç†")
print("="*60)

# 1. Markdown è¡¨æ ¼è½‰ CSV
print("\nã€è™•ç† Markdown è¡¨æ ¼ã€‘")
md_df = markdown_to_csv('table_txt.md', 'output.csv')
if md_df is not None:
    print(f"âœ… Markdown è¡¨æ ¼å·²è½‰æ›ç‚º output.csv")
    print(f"   è¡¨æ ¼å¤§å°: {len(md_df)} è¡Œ x {len(md_df.columns)} åˆ—")

# 2. è®€å– HTML è¡¨æ ¼
print("\nã€è™•ç† HTML è¡¨æ ¼ã€‘")
try:
    tables = pd.read_html("table_html.html", encoding="UTF-8")
    print(f"âœ… å¾ HTML ä¸­è®€å–åˆ° {len(tables)} å€‹è¡¨æ ¼")
    print(f"   è¡¨æ ¼å¤§å°: {tables[0].shape[0]} è¡Œ x {tables[0].shape[1]} åˆ—")
except Exception as e:
    print(f"âŒ è®€å– HTML è¡¨æ ¼å¤±æ•—: {e}")
    tables = None

# 3. ä½¿ç”¨ LLM ç”Ÿæˆè¡¨æ ¼æ‘˜è¦ (Prompt v1)
if tables is not None:
    print("\n" + "="*60)
    print("ä½¿ç”¨ LLM ç”Ÿæˆè¡¨æ ¼æ‘˜è¦ (Prompt v1)...")
    print("="*60)
    
    with open("Prompt_table_v1.txt", "r", encoding="UTF-8") as f:
        system_prompt_v1 = f.read()
    
    client_llm = OpenAI(
        base_url="https://ws-03.wade0426.me/v1",
        api_key="EMPTY",
    )
    
    response_v1 = client_llm.chat.completions.create(
        model="/models/gpt-oss-120b",
        messages=[
            {"role": "system", "content": f"{system_prompt_v1}"},
            {"role": "user", "content": f"{tables[0].to_string()}"}
        ],
        extra_body={
            "chat_template_kwargs": {"enable_thinking": False}
        },
        stream=True
    )
    
    print("\nç”Ÿæˆçš„æ‘˜è¦:")
    print("-"*60)
    table_summary = ""
    for chunk in response_v1:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
            table_summary += content
    
    print("\n" + "-"*60)
    
    # 4. ä½¿ç”¨ LLM ç”Ÿæˆå•ç­”å° (Prompt v2)
    print("\n" + "="*60)
    print("ä½¿ç”¨ LLM ç”Ÿæˆå•ç­”å° (Prompt v2)...")
    print("="*60)
    
    with open("Prompt_table_v2.txt", "r", encoding="UTF-8") as f:
        system_prompt_v2 = f.read()
    
    response_v2 = client_llm.chat.completions.create(
        model="/models/gpt-oss-120b",
        messages=[
            {"role": "system", "content": f"{system_prompt_v2}"},
            {"role": "user", "content": f"{tables[0].to_string()}"}
        ],
        extra_body={
            "chat_template_kwargs": {"enable_thinking": False}
        },
        stream=False
    )
    
    qa_json = response_v2.choices[0].message.content
    print("\nç”Ÿæˆçš„å•ç­”å°:")
    print("-"*60)
    print(qa_json)
    print("-"*60)
    
    # 5. å°‡è¡¨æ ¼æ‘˜è¦å’Œå•ç­”å°å­˜å…¥ Qdrant
    print("\nğŸ’¾ é–‹å§‹å°‡è¡¨æ ¼è³‡æ–™å­˜å…¥ Qdrant...")
    
    # å»ºç«‹è¡¨æ ¼é›†åˆ
    if client.collection_exists("table_collection"):
        client.delete_collection("table_collection")
    
    client.create_collection(
        collection_name="table_collection",
        vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
    )
    
    # æº–å‚™æ‰€æœ‰æ–‡æœ¬ï¼ˆæ‘˜è¦ + å•ç­”å°ï¼‰
    all_table_texts = [table_summary]
    
    # è§£æå•ç­”å°
    try:
        import json
        qa_list = json.loads(qa_json)
        for qa in qa_list:
            qa_text = f"å•é¡Œ: {qa['question']}\nç­”æ¡ˆ: {qa['answer']}"
            all_table_texts.append(qa_text)
        print(f"âœ… æˆåŠŸè§£æ {len(qa_list)} çµ„å•ç­”å°")
    except:
        print("âš ï¸  å•ç­”å°æ ¼å¼è§£æå¤±æ•—ï¼Œåƒ…ä¿ç•™æ‘˜è¦")
    
    # ç”Ÿæˆå‘é‡
    table_embed_data = {
        "texts": all_table_texts,
        "normalize": True,
        "batch_size": 32
    }
    table_embed_response = requests.post(API_EMBED_URL, json=table_embed_data)
    
    if table_embed_response.status_code == 200:
        table_embed_result = table_embed_response.json()
        
        # å­˜å…¥ Qdrant
        table_points = []
        for i, vec in enumerate(table_embed_result['embeddings']):
            point_type = "table_summary" if i == 0 else "table_qa"
            table_points.append(
                PointStruct(
                    id=i + 1,
                    vector=vec,
                    payload={
                        "text": all_table_texts[i],
                        "type": point_type,
                        "source": "table_html.html"
                    }
                )
            )
        
        client.upsert(collection_name="table_collection", points=table_points)
        print(f"âœ… æˆåŠŸä¸Šå‚³ {len(table_points)} å€‹è¡¨æ ¼ç›¸é—œè³‡æ–™åˆ° Qdrant")
        print(f"   - 1 å€‹è¡¨æ ¼æ‘˜è¦")
        print(f"   - {len(table_points)-1} å€‹å•ç­”å°")
    else:
        print("âŒ è¡¨æ ¼å‘é‡ç”Ÿæˆå¤±æ•—")

# ============================================
# ç¬¬å››éƒ¨åˆ†ï¼šè¡¨æ ¼æŸ¥è©¢æ¸¬è©¦
# ============================================

print("\n" + "="*60)
print("ç¬¬å››éƒ¨åˆ†ï¼šè¡¨æ ¼æŸ¥è©¢æ¸¬è©¦")
print("="*60)

table_queries = ["å°ä¸­ç§‘å¤§æœ‰ä»€éº¼ç‰¹è‰²?", "å­¸æ ¡çš„ç™¼å±•è¨ˆç•«æ˜¯ä»€éº¼?"]

for query_text in table_queries:
    print(f"\nğŸ” æŸ¥è©¢å•é¡Œ: {query_text}")
    print("-"*60)
    
    query_data = {
        "texts": [query_text],
        "normalize": True,
        "batch_size": 32
    }
    query_response = requests.post(API_EMBED_URL, json=query_data)
    
    if query_response.status_code == 200:
        query_vector = query_response.json()['embeddings'][0]
        
        search_result = client.query_points(
            collection_name="table_collection",
            query=query_vector,
            limit=3
        )
        
        for idx, point in enumerate(search_result.points, 1):
            print(f"\nçµæœ {idx}:")
            print(f"  é¡å‹: {point.payload['type']}")
            print(f"  ç›¸ä¼¼åº¦: {point.score:.4f}")
            print(f"  å…§å®¹: {point.payload['text'][:150]}...")

