import os
import pandas as pd
import requests
import json
import re
import numpy as np
import time
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker

class CustomEmbeddings:
    def embed_documents(self, texts):
        return get_embeddings(texts)
    def embed_query(self, text):
        return get_embeddings([text])[0]

# ============================================
# é…ç½®å€
# ============================================
API_EMBED_URL = "https://ws-04.wade0426.me/embed"
QDRANT_URL = "http://localhost:6333"
SERVER_URL = "https://hw-01.wade0426.me/submit_answer"

client = QdrantClient(url=QDRANT_URL)

# åˆ‡å¡Šåƒæ•¸
chunk_size = 500
chunk_overlap = 250

# èªæ„åˆ‡å¡Šåƒæ•¸ï¼ˆå¯èª¿æ•´ï¼‰
SEMANTIC_THRESHOLD = 0.5  # 0.3=åˆ‡å¾ˆç´°, 0.5=ä¸­ç­‰, 0.7=åˆ‡å¾ˆç²—

# ============================================
# å·¥å…·å‡½æ•¸
# ============================================

def get_embeddings(texts, max_retries=3):
    """å‘¼å« API å–å¾— embeddingsï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            res = requests.post(API_EMBED_URL, json={"texts": texts, "normalize": True}, timeout=30)
            if res.status_code == 200:
                return res.json()['embeddings']
            else:
                print(f"âš ï¸  API å›å‚³ {res.status_code}, é‡è©¦ {attempt+1}/{max_retries}...")
                time.sleep(1)
        except Exception as e:
            print(f"âŒ API å‘¼å«éŒ¯èª¤: {e}, é‡è©¦ {attempt+1}/{max_retries}...")
            time.sleep(1)
    
    print(f"âŒ API å‘¼å«å¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡")
    return None

def submit_homework_and_get_score(q_id, answer):
    payload = {"q_id": q_id, "student_answer": answer}
    try:
        payload["student_answer"] = answer[:2000]
        response = requests.post(SERVER_URL, json=payload)
        return response.json().get('score', 0) if response.status_code == 200 else 0
    except:
        return 0

def setup_collection(name, chunks, payloads):
    if client.collection_exists(name):
        client.delete_collection(name)
    client.create_collection(
        collection_name=name,
        vectors_config=VectorParams(size=4096, distance=Distance.COSINE),
    )
    vecs = get_embeddings(chunks)
    if not vecs:
        print(f"âŒ ç„¡æ³•ç‚ºé›†åˆ {name} å»ºç«‹ embeddings")
        return
    points = [PointStruct(id=i, vector=vecs[i], payload=payloads[i]) for i in range(len(chunks))]
    client.upsert(collection_name=name, points=points)

def semantic_chunking(text, threshold=0.5):
    """çœŸæ­£çš„èªæ„åˆ‡å¡Šå¯¦ä½œ"""
    # 1. æŒ‰æ¨™é»ç¬¦è™Ÿåˆ‡æˆå¥å­
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
    sentences = [''.join(sentences[i:i+2]).strip() for i in range(0, len(sentences)-1, 2) if i+1 < len(sentences)]
    sentences = [s for s in sentences if len(s) > 5]
    
    if len(sentences) == 0:
        return [text]
    if len(sentences) == 1:
        return sentences
    
    # 2. è¨ˆç®—æ¯å€‹å¥å­çš„ embedding
    embeddings = get_embeddings(sentences)
    
    if not embeddings:
        # å›é€€ç­–ç•¥ï¼šç°¡å–®åˆ‡å¡Š
        chunks = []
        for i in range(0, len(text), 500):
            chunks.append(text[i:i+500])
        return chunks
    
    embeddings = np.array(embeddings)
    
    # 3. è¨ˆç®—ç›¸é„°å¥å­çš„é¤˜å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(embeddings) - 1):
        sim = float(np.dot(embeddings[i], embeddings[i+1]))
        similarities.append(sim)
    
    if not similarities:
        return [text]
    
    # 4. æ‰¾å‡ºç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼çš„åˆ‡åˆ†é»
    split_points = []
    for i, sim in enumerate(similarities):
        if sim < threshold:
            split_points.append(i + 1)
    
    # 5. æ ¹æ“šåˆ‡åˆ†é»çµ„åˆå¥å­æˆå€å¡Š
    chunks = []
    start = 0
    for split_point in split_points:
        chunk = ''.join(sentences[start:split_point])
        if chunk.strip():
            chunks.append(chunk)
        start = split_point
    
    last_chunk = ''.join(sentences[start:])
    if last_chunk.strip():
        chunks.append(last_chunk)
    
    return chunks if chunks else [text]

# ============================================
# ä¸»ç¨‹å¼
# ============================================

def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    questions_path = os.path.join(base_dir, "questions.csv")
    data_files = [os.path.join(base_dir, f"data_0{i}.txt") for i in range(1, 6)]

    if not os.path.exists(questions_path):
        print("âŒ æ‰¾ä¸åˆ° questions.csvï¼Œè«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘ï¼")
        return

    df = pd.read_csv(questions_path)
    df.columns = [c.strip().lower() for c in df.columns]
    df = df.rename(columns={'questions': 'question', 'question_id': 'q_id', 'id': 'q_id'})

    all_results = []
    
    custom_emb = CustomEmbeddings()
    
    fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)
    sliding_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    method_map = {
        "å›ºå®šå¤§å°": ("fixed", fixed_splitter),
        "æ»‘å‹•è¦–çª—": ("sliding", sliding_splitter),
        "èªæ„åˆ‡å¡Š": ("semantic", None)
    }

    print(f"âœ¨ é–‹å§‹åŸ·è¡Œ RAG ä½œæ¥­æµç¨‹ âœ¨")
    print("=" * 60)

    for m_name, (m_type, splitter) in method_map.items():
        print(f"ğŸš€ æ­£åœ¨åŸ·è¡Œæ–¹æ³•ï¼š{m_name} ...")
        all_chunks, all_payloads = [], []
        
        for f_path in data_files:
            if not os.path.exists(f_path): continue
            with open(f_path, "r", encoding="utf-8") as f:
                content = f.read()
                
                if m_type == "semantic":
                    chunks = semantic_chunking(content, threshold=SEMANTIC_THRESHOLD)
                else:
                    chunks = splitter.split_text(content)
                
                all_chunks.extend(chunks)
                for c in chunks:
                    all_payloads.append({"text": c, "source": os.path.basename(f_path)})

        print(f"   ğŸ“¦ {m_name} ç¸½å…±åˆ‡å‡º {len(all_chunks)} å€‹å€å¡Š")
        coll_name = f"hw5_{m_name.encode('utf-8').hex()}"
        setup_collection(coll_name, all_chunks, all_payloads)

        method_score = 0
        q_count = 0
        for _, row in df.iterrows():
            q_text = str(row['question'])
            q_id = row['q_id']
            
            q_vec_result = get_embeddings([q_text])
            if not q_vec_result:
                print(f"  âŒ Q{q_id}: embedding å¤±æ•—ï¼Œè·³é")
                continue
            
            q_vec = q_vec_result[0]
            
            try:
                search_res = client.query_points(collection_name=coll_name, query=q_vec, limit=3).points
            except Exception as e:
                print(f"  âŒ Q{q_id}: æœå°‹å¤±æ•— - {e}")
                continue
            
            if search_res:
                combined_answer = "\n".join([res.payload['text'] for res in search_res])
                score = submit_homework_and_get_score(q_id, combined_answer)
                source = search_res[0].payload['source']
                method_score += score
                q_count += 1
                
                print(f"  ğŸ”¹ Q{q_id} ({m_name}): åˆ†æ•¸ {score:.4f}, ä¾†æº {source}")

                all_results.append({
                    "id": len(all_results) + 1,
                    "q_id": q_id,
                    "method": m_name,
                    "retrieve_text": combined_answer,
                    "score": score,
                    "source": source
                })
        
        avg_score = method_score / q_count if q_count > 0 else 0
        print(f"  âœ… {m_name} åŸ·è¡Œå®Œç•¢ï¼Œç¸½å¾—åˆ†: {method_score:.4f}ï¼Œå¹³å‡å¾—åˆ†: {avg_score:.4f}\n")

    final_df = pd.DataFrame(all_results)
    final_output = os.path.join(base_dir, "1111132028_RAG_HW_01.csv")
    final_df.to_csv(final_output, index=False, encoding="utf-8-sig")

    print("=" * 60)
    summary = final_df.groupby('method')['score'].agg(['sum', 'mean']).sort_values(by='sum', ascending=False)
    
    for method, stats in summary.iterrows():
        print(f"ğŸ“Š æ–¹æ³•ï¼š{method:10} | ç¸½åˆ†ï¼š{stats['sum']:.4f} | å¹³å‡åˆ†ï¼š{stats['mean']:.4f}")
    
    print("-" * 60)
    best_method = summary.index[0]
    print(f"ğŸ† è¡¨ç¾æœ€å¥½çš„æ–¹æ³•ï¼š{best_method}")
    print("=" * 60)

if __name__ == "__main__":
    main()